{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from gpt4all import GPT4All\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../funcs'))\n",
    "import functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import and process the RGB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/chen700564/RGB/master/data/en.json\"\n",
    "data = fn.process_json(url)\n",
    "data = random.sample(data, 5)\n",
    "queries = [item[\"query\"] for item in data]\n",
    "answers = [item[\"answer\"][0] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Extractive Open Source Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load config and mapping dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config\n",
    "with open(\"../config/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load the model mapping\n",
    "with open('../config/models_mapping.json', 'r') as f:\n",
    "    model_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Set up local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up noise thresholds\n",
    "stride = config['globals']['stride']\n",
    "noise_thresholds = fn.get_noise_levels(stride)\n",
    "\n",
    "# Set up device\n",
    "device = GPT4All.list_gpus()[0]\n",
    "\n",
    "# Set up models\n",
    "gen_model_1 = GPT4All(config['generative_models_open_source']['gen_model_1'], device=device)\n",
    "gen_model_2 = GPT4All(config['generative_models_open_source']['gen_model_2'], device=device)\n",
    "gen_model_3 = GPT4All(config['generative_models_open_source']['gen_model_3'], device=device)\n",
    "gen_model_4 = GPT4All(config['generative_models_open_source']['gen_model_4'], device=device)\n",
    "models = [ gen_model_1, gen_model_2, gen_model_3, gen_model_4 ]\n",
    "\n",
    "# Set up models name\n",
    "gen_model_1.name = \"gpt4all.gguf\"\n",
    "gen_model_2.name = \"Meta-Llama.gguf\"\n",
    "gen_model_3.name = \"Nous-Hermes.gguf\"\n",
    "gen_model_4.name = \"Phi.gguf\"\n",
    "\n",
    "# Set up separator\n",
    "separator = config['globals']['separator']\n",
    "\n",
    "# Set up max tokens\n",
    "max_tokens = config['globals']['max_tokens']\n",
    "\n",
    "# Number of experiments\n",
    "num_experiments = config['globals']['num_experiments']\n",
    "\n",
    "# Set up paths\n",
    "input_paths = config['generative_models_open_source']['input_paths']\n",
    "output_paths = config['generative_models_open_source']['output_paths']\n",
    "os.makedirs(input_paths, exist_ok=True)\n",
    "os.makedirs(output_paths, exist_ok=True)\n",
    "\n",
    "# Set up prompt\n",
    "with open(\"../config/prompts/prompt.txt\", \"r\") as f:\n",
    "    prompt_template = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_times = []\n",
    "overall_times = []\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    model_start_time = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    loaded_model = AutoModelForQuestionAnswering.from_pretrained(model)\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=loaded_model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "    model_times = []\n",
    "    \n",
    "    for exp_num in range(1, num_experiments + 1):\n",
    "\n",
    "        random.seed(2024 + exp_num)\n",
    "\n",
    "        results = []\n",
    "        exp_start_time = time.time()\n",
    "        for query, positive_context, negative_context, answer in tqdm(zip(queries, [item[\"positive\"] for item in data], [item[\"negative\"] for item in data], answers), total=len(queries)):\n",
    "            result = {\n",
    "                'Query': query,\n",
    "                'Correct Answer': answer,\n",
    "            }\n",
    "            for noise_level, value in noise_thresholds.items():\n",
    "                noise_start_time = time.time()\n",
    "                \n",
    "                with model.chat_session():\n",
    "                    mixed_context = fn.create_mixed_context(positive_context, negative_context, value, max_tokens, separator)\n",
    "                    context_concat = separator.join(mixed_context)\n",
    "                    prompt = prompt_template.format(context_concat=context_concat, query=query)\n",
    "                    generated_answer = model.generate(prompt)\n",
    "                    result.update({f'{noise_level} Predicted Answer': generated_answer})\n",
    "                    result[f'Jaccard {noise_level}'] = fn.apply_jaccard(result, f'{noise_level} Predicted Answer', 'Correct Answer')\n",
    "                    result[f'Cosine {noise_level}'] = fn.apply_cosine(result, f'{noise_level} Predicted Answer', 'Correct Answer')\n",
    "                    result[f'EM {noise_level}'] = fn.apply_exact_match(result, f'{noise_level} Predicted Answer', 'Correct Answer')\n",
    "                    result[f'EM - 2V {noise_level}'] = fn.apply_exact_match_2v(result, f'{noise_level} Predicted Answer', 'Correct Answer')\n",
    "                    \n",
    "                    noise_end_time = time.time()\n",
    "                    noise_times = noise_end_time - noise_start_time\n",
    "                    \n",
    "                    execution_times.append({\n",
    "                        'Model': model,\n",
    "                        'Noise Level': noise_level,\n",
    "                        'Average Time': noise_times,\n",
    "                        'Standard Deviation': 0\n",
    "                    })\n",
    "\n",
    "            results.append(result)\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        filename_results = os.path.join(input_paths, f\"exp_{exp_num}_{model.name}.json\")\n",
    "        results_df.to_json(filename_results, orient='records', lines=True)\n",
    "        exp_end_time = time.time()\n",
    "        model_times.append(exp_end_time - exp_start_time)\n",
    "    \n",
    "    model_end_time = time.time()\n",
    "    \n",
    "    avg_time = np.mean(model_times)\n",
    "    std_time = np.std(model_times)\n",
    "\n",
    "    print(f\"Execution time for model {model}: {(model_end_time - model_start_time) / 60:.2f} minutes.\")\n",
    "\n",
    "    overall_times.append({\n",
    "        'Model': model,\n",
    "        'Average Time': avg_time,\n",
    "        'Standard Deviation': std_time\n",
    "    })\n",
    "\n",
    "execution_times_df = pd.DataFrame(execution_times)\n",
    "overall_times_df = pd.DataFrame(overall_times)\n",
    "filename_exec_time = os.path.join(input_paths, \"exec_time.xlsx\")\n",
    "with pd.ExcelWriter(filename_exec_time, engine='xlsxwriter') as writer:\n",
    "    execution_times_df.to_excel(writer, sheet_name='Noise Level Times', index=False)\n",
    "    overall_times_df.to_excel(writer, sheet_name='Overall Model Times', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Compute all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join(output_paths, 'all_metrics.xlsx')\n",
    "all_results = {}\n",
    "\n",
    "files = os.listdir(input_paths)\n",
    "for i, file in enumerate(tqdm(files, desc=\"Processing files\"), start=1):\n",
    "    if file.endswith('.json'):\n",
    "        experiment_num = int(file.split('_')[1]) \n",
    "        model_name = file.split('_')[2].replace('.json', '') \n",
    "        sheet_name = model_mapping.get(model_name, model_name)\n",
    "        input_path = os.path.join(input_paths, file)\n",
    "        result_df = fn.compute_metrics(input_path, stride) \n",
    "        result_df.insert(0, 'Experiment Number', experiment_num)\n",
    "        if sheet_name not in all_results:\n",
    "            all_results[sheet_name] = result_df\n",
    "        else:\n",
    "            all_results[sheet_name] = pd.concat([all_results[sheet_name], result_df], ignore_index=True)\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, result_df in all_results.items():\n",
    "        result_df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Compute mean metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../metrics/gen/all_metrics.xlsx'\n",
    "output_file = '../metrics/gen/final_metrics.xlsx'\n",
    "final_results = {}\n",
    "\n",
    "excel_data = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "for sheet_name, df in excel_data.items():\n",
    "    if 'Experiment Number' not in df.columns:\n",
    "        raise ValueError(f\"'Experiment Number' column not found in sheet {sheet_name}\")\n",
    "    metrics = df['Metric'].unique()\n",
    "    noise_levels = list(noise_thresholds.keys())\n",
    "    result_data = {\n",
    "        'Metric': metrics,\n",
    "    }\n",
    "    for noise_level in noise_levels:\n",
    "        result_data[f'{noise_level}_Mean'] = []\n",
    "        result_data[f'{noise_level}_Std'] = []\n",
    "        for metric in metrics:\n",
    "            metric_df = df[df['Metric'] == metric]\n",
    "            result_data[f'{noise_level}_Mean'].append(metric_df[noise_level].mean())\n",
    "            result_data[f'{noise_level}_Std'].append(metric_df[noise_level].std())\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    final_results[sheet_name] = result_df\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, result_df in final_results.items():\n",
    "        result_df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
