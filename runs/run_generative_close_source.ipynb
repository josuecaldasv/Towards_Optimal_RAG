{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phaxssi/miniconda3/envs/rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath('../funcs'))\n",
    "import functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import and process the RGB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/chen700564/RGB/master/data/en.json\"\n",
    "data = fn.process_json(url)\n",
    "data = random.sample(data, 1)\n",
    "queries = [item[\"query\"] for item in data]\n",
    "answers = [item[\"answer\"][0] for item in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Extractive Open Source Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load config and mapping dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config\n",
    "with open(\"../config/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load the model mapping\n",
    "with open('../config/models_mapping.json', 'r') as f:\n",
    "    model_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Set up local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up noise thresholds\n",
    "stride = config['globals']['stride']\n",
    "noise_thresholds = fn.get_noise_levels(stride)\n",
    "\n",
    "# Set up device\n",
    "# device = GPT4All.list_gpus()[0]\n",
    "\n",
    "# Set up the client\n",
    "client = OpenAI()\n",
    "\n",
    "# Set up models\n",
    "gen_model_1 = config['generative_models_closed_source']['gen_model_1']\n",
    "gen_model_2 = config['generative_models_closed_source']['gen_model_2']\n",
    "models = [ gen_model_1, gen_model_2 ]\n",
    "\n",
    "# Set up separator\n",
    "separator = config['globals']['separator']\n",
    "\n",
    "# Set up max tokens\n",
    "max_tokens = config['globals']['max_tokens']\n",
    "\n",
    "# Number of experiments\n",
    "num_experiments = config['globals']['num_experiments']\n",
    "\n",
    "# Set up paths\n",
    "input_paths = config['generative_models_closed_source']['input_paths']\n",
    "output_paths = config['generative_models_closed_source']['output_paths']\n",
    "os.makedirs(input_paths, exist_ok=True)\n",
    "os.makedirs(output_paths, exist_ok=True)\n",
    "\n",
    "# Set up prompt\n",
    "with open(\"../config/prompts/generative_closed_source.txt\", \"r\") as f:\n",
    "    prompt_template = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.43s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.04s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.95s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for model gpt-3.5-turbo: 0.42 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.54s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.42s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.25s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.21s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for model gpt-4o-mini: 0.36 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "execution_times = []\n",
    "overall_times = []\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    model_start_time = time.time()\n",
    "\n",
    "    model_times = []\n",
    "    \n",
    "    for exp_num in range(1, num_experiments + 1):\n",
    "\n",
    "        random.seed(2024 + exp_num)\n",
    "\n",
    "        results = []\n",
    "        exp_start_time = time.time()\n",
    "        for query, positive_context, negative_context, answer in tqdm(zip(queries, [item[\"positive\"] for item in data], [item[\"negative\"] for item in data], answers), total=len(queries)):\n",
    "            result = {\n",
    "                'Query': query,\n",
    "                'Correct Answer': answer,\n",
    "            }\n",
    "            for noise_level, value in noise_thresholds.items():\n",
    "                noise_start_time = time.time()\n",
    "                \n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": prompt_template},\n",
    "                        {\"role\": \"user\", \"content\": f\"This is the question: {query}. Consider the following context: {positive_context}\"}\n",
    "                    ]\n",
    "                )\n",
    "                generated_answer = completion.choices[0].message.content\n",
    "                result.update({f'{noise_level} Predicted Answer': generated_answer})\n",
    "                result[f'Jaccard {noise_level}'] = fn.apply_jaccard(result, f'{noise_level} Predicted Answer', 'Correct Answer')\n",
    "                result[f'Cosine {noise_level}'] = fn.apply_cosine(result, f'{noise_level} Predicted Answer', 'Correct Answer')\n",
    "                result[f'EM {noise_level}'] = fn.apply_exact_match(result, f'{noise_level} Predicted Answer', 'Correct Answer')\n",
    "                result[f'EM - 2V {noise_level}'] = fn.apply_exact_match_2v(result, f'{noise_level} Predicted Answer', 'Correct Answer')\n",
    "                \n",
    "                noise_end_time = time.time()\n",
    "                noise_times = noise_end_time - noise_start_time\n",
    "                \n",
    "                execution_times.append({\n",
    "                    'Model': model,\n",
    "                    'Noise Level': noise_level,\n",
    "                    'Average Time': noise_times,\n",
    "                    'Standard Deviation': 0\n",
    "                })\n",
    "\n",
    "            results.append(result)\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        filename_results = os.path.join(input_paths, f\"exp_{exp_num}_{model}.json\")\n",
    "        results_df.to_json(filename_results, orient='records', lines=True)\n",
    "        exp_end_time = time.time()\n",
    "        model_times.append(exp_end_time - exp_start_time)\n",
    "    \n",
    "    model_end_time = time.time()\n",
    "    \n",
    "    avg_time = np.mean(model_times)\n",
    "    std_time = np.std(model_times)\n",
    "\n",
    "    print(f\"Execution time for model {model}: {(model_end_time - model_start_time) / 60:.2f} minutes.\")\n",
    "\n",
    "    overall_times.append({\n",
    "        'Model': model,\n",
    "        'Average Time': avg_time,\n",
    "        'Standard Deviation': std_time\n",
    "    })\n",
    "\n",
    "execution_times_df = pd.DataFrame(execution_times)\n",
    "overall_times_df = pd.DataFrame(overall_times)\n",
    "filename_exec_time = os.path.join(input_paths, \"exec_time.xlsx\")\n",
    "with pd.ExcelWriter(filename_exec_time, engine='xlsxwriter') as writer:\n",
    "    execution_times_df.to_excel(writer, sheet_name='Noise Level Times', index=False)\n",
    "    overall_times_df.to_excel(writer, sheet_name='Overall Model Times', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Compute all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 11/11 [00:07<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "output_file = os.path.join(output_paths, 'all_metrics.xlsx')\n",
    "all_results = {}\n",
    "\n",
    "files = os.listdir(input_paths)\n",
    "for i, file in enumerate(tqdm(files, desc=\"Processing files\"), start=1):\n",
    "    if file.endswith('.json'):\n",
    "        experiment_num = int(file.split('_')[1]) \n",
    "        model_name = file.split('_')[2].replace('.json', '') \n",
    "        sheet_name = model_mapping.get(model_name, model_name)\n",
    "        input_path = os.path.join(input_paths, file)\n",
    "        result_df = fn.compute_metrics(input_path, stride) \n",
    "        result_df.insert(0, 'Experiment Number', experiment_num)\n",
    "        if sheet_name not in all_results:\n",
    "            all_results[sheet_name] = result_df\n",
    "        else:\n",
    "            all_results[sheet_name] = pd.concat([all_results[sheet_name], result_df], ignore_index=True)\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, result_df in all_results.items():\n",
    "        result_df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Compute mean metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../metrics/gen/closed/all_metrics.xlsx'\n",
    "output_file = '../metrics/gen/closed/final_metrics.xlsx'\n",
    "final_results = {}\n",
    "\n",
    "excel_data = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "for sheet_name, df in excel_data.items():\n",
    "    if 'Experiment Number' not in df.columns:\n",
    "        raise ValueError(f\"'Experiment Number' column not found in sheet {sheet_name}\")\n",
    "    metrics = df['Metric'].unique()\n",
    "    noise_levels = list(noise_thresholds.keys())\n",
    "    result_data = {\n",
    "        'Metric': metrics,\n",
    "    }\n",
    "    for noise_level in noise_levels:\n",
    "        result_data[f'{noise_level}_Mean'] = []\n",
    "        result_data[f'{noise_level}_Std'] = []\n",
    "        for metric in metrics:\n",
    "            metric_df = df[df['Metric'] == metric]\n",
    "            result_data[f'{noise_level}_Mean'].append(metric_df[noise_level].mean())\n",
    "            result_data[f'{noise_level}_Std'].append(metric_df[noise_level].std())\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    final_results[sheet_name] = result_df\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, result_df in final_results.items():\n",
    "        result_df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
